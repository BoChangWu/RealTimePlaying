{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bccbd1f",
   "metadata": {},
   "source": [
    "參考範例: [AutoVC實作(Pytorch)](https://ithelp.ithome.com.tw/m/articles/10262975)\n",
    "\n",
    "Dataset: [VCTK dataset](https://datashare.ed.ac.uk/handle/10283/2950)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87ee05",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2450fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from scipy import signal\n",
    "from scipy.signal import get_window\n",
    "from librosa.filters import mel\n",
    "from librosa.util import normalize\n",
    "from numpy.random import RandomState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d2e45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: ./DR-VCTK/DR-VCTK/DR-VCTK\n",
      "['clean_testset_wav_16k', 'clean_trainset_wav_16k', 'device-recorded_testset_wav_16k', 'device-recorded_trainset_wav_16k']\n"
     ]
    }
   ],
   "source": [
    "# data dir\n",
    "rootDir = './DR-VCTK/DR-VCTK/DR-VCTK'\n",
    "\n",
    "# mel storage dir\n",
    "targetDir = './spmel'\n",
    "dirName, subdirList,_ = next(os.walk(rootDir))\n",
    "print('Found directory: %s' % dirName)\n",
    "print(subdirList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aaa3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_gan_handler(x, fft_length=1024, hop_length=256, sr=22050):\n",
    "    wav = normalize(x)\n",
    "    p = (fft_length - hop_length) // 2\n",
    "    wav = np.squeeze(np.pad(wav, (p,p), 'reflect'))\n",
    "    fft = librosa.stft(\n",
    "                        wav,\n",
    "                        n_fft=fft_length,\n",
    "                        hop_length = hop_length,\n",
    "                        window = 'hann',\n",
    "                        center = False\n",
    "                        )\n",
    "    \n",
    "    # 這裡的 abs 是 sqrt(實部**2 + 虛部**2)\n",
    "    mag = abs(fft)\n",
    "    mel_basis = mel(sr, 1024, fmin = 0.0, fmax=None, n_mels=80)\n",
    "    mel_output = np.dot(mel_basis, mag)\n",
    "    log_mel_spec = np.log10(np.maximum(1e-5,mel_output)).astype(np.float32)\n",
    "    return log_mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "263c679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\AppData\\Local\\Temp/ipykernel_17992/1697858801.py:16: FutureWarning: Pass orig_sr=16000, target_sr=22050 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  x = librosa.resample(x, fs, new_rate)\n",
      "C:\\Users\\jason\\AppData\\Local\\Temp/ipykernel_17992/771229521.py:15: FutureWarning: Pass sr=22050, n_fft=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = mel(sr, 1024, fmin = 0.0, fmax=None, n_mels=80)\n",
      "C:\\Users\\jason\\AppData\\Local\\Temp/ipykernel_17992/1697858801.py:16: FutureWarning: Pass orig_sr=16000, target_sr=22050 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  x = librosa.resample(x, fs, new_rate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done --- clean_testset_wav_16k\n",
      "Done --- clean_trainset_wav_16k\n",
      "Done --- device-recorded_testset_wav_16k\n",
      "Done --- device-recorded_trainset_wav_16k\n"
     ]
    }
   ],
   "source": [
    "# VCTK 是 48KHz 我們必須先 resample 到 22.05 KHz\n",
    "new_rate = 22050\n",
    "\n",
    "for subdir in sorted(subdirList):\n",
    "    if not os.path.exists(os.path.join(targetDir, subdir)):\n",
    "        os.makedirs(os.path.join(targetDir, subdir))\n",
    "    \n",
    "    _,_, fileList= next(os.walk(os.path.join(dirName,subdir)))\n",
    "    \n",
    "    for fileName in sorted(fileList):\n",
    "        x, fs = sf.read(os.path.join(dirName,subdir,fileName))\n",
    "        ##############\n",
    "        # change sample rate from 48000 -> 22050\n",
    "        # Since mel_gan use 22050\n",
    "        ##############\n",
    "        x = librosa.resample(x, fs, new_rate)\n",
    "        S = mel_gan_handler(x)\n",
    "        \n",
    "        np.save(os.path.join(targetDir, subdir, fileName[:-5]), \n",
    "               S.astype(np.float32), allow_pickle=False)\n",
    "        \n",
    "    print(f\"Done --- {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73e47e",
   "metadata": {},
   "source": [
    "# 生成 D_VECTOR\n",
    "\n",
    "下載 [Pre_train Model](https://drive.google.com/file/d/1ORAeb4DlS_65WDkQN6LHx5dPyCM5PAVV/view) 後定義 D_VECTOR, 這裡用的是 LSTM版的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb705f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class D_VECTOR(nn.Module):\n",
    "    \"\"\" D_VECTOR speaker embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
    "        super(D_VECTOR, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=dim_input, hidden_size=dim_cell,\n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "        self.embedding = nn.Linear(dim_cell, dim_emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        embeds = self.embedding(lstm_out[:,-1,:])\n",
    "        norm = embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "        embeds_normalized = embeds.div(norm)\n",
    "        return embeds_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8305e5b",
   "metadata": {},
   "source": [
    "接著要 Load_state_dict, 注意 num_uttrs 這個參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0c2abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: ./spmel\n",
      "Processing speaker: clean_trainset_wav_16k\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 80, got 342",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m             left \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, tmp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mlen_crop)\n\u001b[0;32m     60\u001b[0m             melsp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(tmp[np\u001b[38;5;241m.\u001b[39mnewaxis, left:left\u001b[38;5;241m+\u001b[39mlen_crop, :])\n\u001b[1;32m---> 62\u001b[0m         emb \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmelsp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m         embs\u001b[38;5;241m.\u001b[39mappend(emb,detach()\u001b[38;5;241m.\u001b[39msqueeze\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     65\u001b[0m utterance\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(embs,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mD_VECTOR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[1;32m---> 15\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(lstm_out[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n\u001b[0;32m     17\u001b[0m     norm \u001b[38;5;241m=\u001b[39m embeds\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\rnn.py:692\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    691\u001b[0m                        ):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    694\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 80, got 342"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# from model_bl import D_VECTOR\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cpu()\n",
    "c_checkpoint = torch.load('3000000-BL.ckpt',map_location=torch.device('cpu'))\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for key, val in c_checkpoint['model_b'].items():\n",
    "    new_key = key[7:]\n",
    "    new_state_dict[new_key] = val\n",
    "    \n",
    "C.load_state_dict(new_state_dict)\n",
    "\n",
    "# 指的是說一個語者說了幾種不同內容的話, 讓資料的數量盡量一樣, 內容可以不一樣\n",
    "num_uttrs = 68\n",
    "len_crop = 176\n",
    "\n",
    "# Directory containing mel-spectrograms\n",
    "rootDir = './spmel'\n",
    "dirName, subdirList, _ = next(os.walk(rootDir))\n",
    "print('Found directory: %s' % dirName)\n",
    "\n",
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int=0):\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    \n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "    npad = [(0,0)] * array.ndim\n",
    "    npad[axis] = (0,pad_size)\n",
    "    \n",
    "    return np.pad(array, pad_size)\n",
    "\n",
    "speakers = []\n",
    "for speaker in sorted(subdirList[1:]):\n",
    "    print('Processing speaker: %s' % speaker)\n",
    "    utterances = []\n",
    "    utterances.append(speaker)\n",
    "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
    "    fileList = fileList[:num_uttrs]\n",
    "    \n",
    "    # make speaker embedding\n",
    "    assert len(fileList) >= num_uttrs\n",
    "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
    "    embs = []\n",
    "\n",
    "    for i in range(num_uttrs):\n",
    "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]),allow_pickle=True)\n",
    "        \n",
    "        # pad if the current one is too short\n",
    "        if tmp.shape[0] <= len_crop:\n",
    "            pad =int(len_crop - tmp.shape[0])\n",
    "            tmp = pad_along_axis(tmp,pad)\n",
    "            melsp = torch.from_numpy(tmp[np.newaxis,:, :])\n",
    "        else:\n",
    "            left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
    "            melsp = torch.from_numpy(tmp[np.newaxis, left:left+len_crop, :])\n",
    "        \n",
    "        emb = C(melsp)\n",
    "        embs.append(emb,detach().squeeze.cpu().numpy())\n",
    "        \n",
    "utterance.append(np.mean(embs,axis=0))\n",
    "for fileName in sorted(fileList):\n",
    "    utterances.append(os.path.join(speaker,fileName))\n",
    "speakers.append(utterance)\n",
    "\n",
    "with open(os.path.join(rootDir,'train.pkl'),'wb') as handle:\n",
    "    pickle.dump(speakers,handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99640a99",
   "metadata": {},
   "source": [
    "# 製作 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eced095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "class Utterances(data.Dataset):\n",
    "    \"\"\"Data class for the Utterances dataset\"\"\"\n",
    "    \n",
    "    def __init__(self,root_dir,len_crop):\n",
    "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.len_crop = len_crop\n",
    "        self.step = 10\n",
    "        \n",
    "        metaname = os.path.join(self.root_dir, \"train.pkl\")\n",
    "        meta = pickle.load(open(metaname,'rb'))\n",
    "        \n",
    "        \"\"\"Load data using multiprocessing\"\"\"\n",
    "        manager = Manager()\n",
    "        meta = manager.list(meta)\n",
    "        dataset = manager.list(len(meta)*[None])\n",
    "        processes = []\n",
    "        \n",
    "        for i in range(0,len(meta), self.step):\n",
    "            p = Process(target=self.load_data,\n",
    "                        args =(meta[i:i+self.step],dataset,i))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            \n",
    "        self.train_dataset = list(dataset)\n",
    "        self.num_tokens = len(self.train_dataset)\n",
    "        \n",
    "        print('Finished loading the dataset...')\n",
    "        \n",
    "    def load_data(self,submeta,dataset,idx_offset):\n",
    "        for k, sbmt in enumerate(submeta):\n",
    "            uttrs = len(sbmt)*[None]\n",
    "            for j, tmp in enumerate(sbmt):\n",
    "                if j<2: # fill in speaker id and embedding\n",
    "                    uttrs[j] = tmp\n",
    "                else: # load the mel-spectrograms\n",
    "                    uttrs[j] = np.load(os.path.join(self.root_dir,tmp))\n",
    "            dataset[idx_offset+k] = uttrs\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        #pick a random speaker\n",
    "        dataset = self.train_dataset\n",
    "        list_uttrs =dataset[index]\n",
    "        emb_org = list_uttrs[1]\n",
    "        \n",
    "        # pick random uttr with random crop\n",
    "        a = np.random.randint(2,len(list_uttrs))\n",
    "        tmp = list_uttrs[a]\n",
    "        \n",
    "        if tmp.shape[0]<self.len_crop:\n",
    "            len_pad = self.len_crop - tmp.shape[0]\n",
    "            uttr = np.pad(tmp, ((0,len_pad),(0,0)),'constant')\n",
    "        elif tmp.shape[0]> self.len_crop:\n",
    "            left = np.random.randint(tmp.shape[0]-self.len_crop)\n",
    "            uttr = tmp[left:left+self.len_crop, :]\n",
    "        else:\n",
    "            uttr = tmp\n",
    "            \n",
    "        return uttr, emb_org\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of speakers.\"\"\"\n",
    "        return self.num_tokens\n",
    "    \n",
    "def get_loader(root_dir,batch_size=2, len_crop=176,num_workers=0):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    \n",
    "    dataset = Uttrances(root_dir,len_crop)\n",
    "    \n",
    "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers = num_workers,\n",
    "                                  drop_last=True,\n",
    "                                  worker_init_fn = worker_init_fn)\n",
    "    return data_loader\n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964d81a",
   "metadata": {},
   "source": [
    "使用時只需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7dee647",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vcc_loader \u001b[38;5;241m=\u001b[39m get_loader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./spmel\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mBATCH_SIZE\u001b[49m,LEN_CROP)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(step):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "vcc_loader = get_loader('./spmel',BATCH_SIZE,LEN_CROP)\n",
    "\n",
    "for j in range(step):\n",
    "    try:\n",
    "        x_real, emb_org = next(data_iter)\n",
    "    except:\n",
    "        data_iter = iter(vcc_loader)\n",
    "        x_real, emb_org = next(data_iter)\n",
    "        '''\n",
    "        train model here\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b4a0e5",
   "metadata": {},
   "source": [
    "# AutoVC\n",
    "先參考官網  [model_vc.py](https://github.com/auspicious3000/autovc/blob/master/model_vc.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4086f0",
   "metadata": {},
   "source": [
    "# 製作 Solver\n",
    "\n",
    "把官網 [solver_encoder.py](https://github.com/auspicious3000/autovc/blob/master/solver_encoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b933517",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 原本會造成 shape miss-match 導致收斂過快無法學習 \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m g_loss_id \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mmse_loss(x_real, x_identic\u001b[38;5;241m.\u001b[39msqueeze())  \n\u001b[0;32m      3\u001b[0m g_loss_id_psnt \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(x_real, x_identic_psnt\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# 原本會造成 shape miss-match 導致收斂過快無法學習 \n",
    "g_loss_id = F.mse_loss(x_real, x_identic.squeeze())  \n",
    "g_loss_id_psnt = F.mse_loss(x_real, x_identic_psnt.squeeze())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d96d8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from solver_encoder import Solver\n",
    "from data_loader import get_loader\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.data_dir = './spmel'\n",
    "        self.batch_size = 2\n",
    "        self.len_crop = 176\n",
    "        self.lambda_cd =1\n",
    "        self.dim_neck = 44\n",
    "        self.dim_emb = 256\n",
    "        self.dim_pre = 512\n",
    "        self.freq = 22\n",
    "        self.num_iters = 1000000\n",
    "        slf.log_step = 10\n",
    "\n",
    "config = Config()\n",
    "vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n",
    "solver = Solver(vcc_loader, config)\n",
    "solver.train()\n",
    "torch.save(solver.G.state_dict(),'autovc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ffe91",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from model_vc import Generator\n",
    "\n",
    "device = 'cuda:0'\n",
    "G = Generator(32,256,512,22).eval().to(device)\n",
    "G.load_state_dict(torch.load('autovc'))\n",
    "metadata = pickle.load(open('spmel/train.pkl','rb'))\n",
    "\n",
    "source = 0\n",
    "target = 3\n",
    "\n",
    "# 自定義 source\n",
    "uttr - np.load(f\"spmel/.../... .npy\")\n",
    "#(1,256)\n",
    "emb_org = torch.from_numpy(np.expand_dims(metadata[source][1],axis=0)).to(device)\n",
    "#(1,256)\n",
    "emb_trg = torch.from_numpy(np.expand_dims(metadata[target][1],axis=0)).to(device)\n",
    "\n",
    "#(1,178,80)\n",
    "uttr = torch.from_numpy(np.expand_dims(uttr,axis=0)).to(device)\n",
    "uttr_trg = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, x_identic_psnt,_ = G(uttr,emb_org, emb_trg)\n",
    "\n",
    "#(176,80)\n",
    "uttr_trg = x_identic_psnt[0,0,L,L].cpy().numpy()\n",
    "\n",
    "# To Waveform\n",
    "from interface import *\n",
    "vocoder = MelVocoder()\n",
    "audio = np.squeeze(vocoder.inverse(torch.from_numpy(np.expand_dims(uttr_trg.T,axis=0))).cpu().numpy())\n",
    "ipd.Audio(audio,rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e712339",
   "metadata": {},
   "source": [
    "# Pytorch with CUDA Problem\n",
    "\n",
    "AssertionError: Torch not compiled with CUDA enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d073004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "befa9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([0.,1.,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12063598",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train\u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "X_train= X_train.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c6f718d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\cuda\\__init__.py:482\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df58d7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ade80",
   "metadata": {},
   "source": [
    "https://www.datasciencelearner.com/assertionerror-torch-not-compiled-with-cuda-enabled-fix/#:~:text=AssertionError%3A%20torch%20not%20compiled%20with%20Cuda%20enabled%20error%20occurs%20because,false%20or%20removing%20the%20same.\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "[Why torch.cuda.is_available() always False](https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with)\n",
    "\n",
    "\n",
    "[淺談Pytorch與 Torch關係](https://oldpan.me/archives/pytorch-torch-relation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5235b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
