{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2762ef3f",
   "metadata": {},
   "source": [
    "[參考](https://ithelp.ithome.com.tw/m/articles/10264243)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edf084",
   "metadata": {},
   "source": [
    "# Content-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6571ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def Encoder(input_shape,dim_neck = 32, dim_emb =256,freq =22):\n",
    "    initializer = tf.keras.initializers.GlorotUniform(tf.sqrt(2.0))\n",
    "    \n",
    "    inp = Input(shape=input_shape)\n",
    "    ### 要把 336 的那維變成 512\n",
    "    ####\n",
    "    x= tf.transpose(inp,(0,2,1))\n",
    "    x= Conv1D(512,kernel_size = 5, strides=1, padding=\"same\",dilation_rate=1,kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    ####\n",
    "    x= tf.transpose(inp,(0,2,1))\n",
    "    x= Conv1D(512,kernel_size = 5, strides=1, padding=\"same\",dilation_rate=1,kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    ####\n",
    "    x= tf.transpose(inp,(0,2,1))\n",
    "    x= Conv1D(512,kernel_size = 5, strides=1, padding=\"same\",dilation_rate=1,kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    ######################\n",
    "    # 對 176 那維做 LSTM  #\n",
    "    ######################\n",
    "    \n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    lstm_tf_1 = LSTM(32,return_sequences = True)\n",
    "    lstm_tf_2 = LSTM(32,return_sequences = True)\n",
    "    \n",
    "    x = Bidrectional(lstm_tf_1)(x)\n",
    "    x = Bidrectional(lstm_tf_2)(x)\n",
    "    \n",
    "    ## 採樣\n",
    "    x_up = x[:,:,:dim_neck]\n",
    "    x_down = x[:,:,dim_neck:]\n",
    "    codes = []\n",
    "    \n",
    "    for i in range(0,LEN_CROP, FREQ):\n",
    "        codes.append(tf.concat((x_up[:,i+freq-1,:],x_down[:,i,:]),axis=-1))\n",
    "        \n",
    "    return Model(inputs=inp, outputs=codes, name=\"content_encoder\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a55c4a",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22c9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(encoder_input_shape):\n",
    "    initializer = tf.keras.initializers.GlorotUniform(tf.sqrt(2.0))\n",
    "    inputs = Input(shape = encoder_input_shape)\n",
    "    \n",
    "    # 進 LSTM 時 shape = (2,176,320)\n",
    "    x = LSTM(512,return_sequences = True,kernel_initializer=initializer)(inputs)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    3 個 5x1 Conv + BN + ReLU\n",
    "    \"\"\"\n",
    "    x = Conv1D(512, kernel_size=5, strides=1, padding=\"same\",dilation_rate=1,kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = Conv1D(512,kernel_size=5, strides=1, padding=\"same\",dilation_rate=1,kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    \n",
    "    ###########################\n",
    "    # 對 512 那維做 LSTM      #\n",
    "    ###########################\n",
    "    \n",
    "    x = LSTM(1024, return_sequences = True, kernel_initializer=initializer)(x)\n",
    "    x = LSTM(1024, return_sequences = True, kernel_initionalizer=inializer)(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    Linear\n",
    "    \"\"\"\n",
    "    \n",
    "    x= Dense(80)(x)\n",
    "    \n",
    "    return Model(inputs=inputs, output=x, name=\"decoder\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c69da5",
   "metadata": {},
   "source": [
    "# Decoder 的第二個輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1236398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Posnet(input_shape):\n",
    "    initializer = tf.keras.initializers.GlorotUniform(1)\n",
    "    inp = Input(shape = input_shape)\n",
    "    \n",
    "    \"\"\"\n",
    "    這裡是第二個輸出\n",
    "    要把 80 的那維變成 512\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    4 個 5x1 Conv + BN + ReLU\n",
    "    \"\"\"\n",
    "    \n",
    "    x = Conv1D(512, kernel_size=5, strides=1, padding=\"same\", dilation_rate=1,kernel_initializer=intializer)(inp)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"tanh\")(x)\n",
    "    \n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = Conv1D(512,kernel_size=5, strides=1,padding=\"same\",dilation_rate=1,kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = Conv1D(512, kernel_size=5, strides=1,padding=\"same\", dilation_rate=1, kernel_initializer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    \n",
    "    x = BatchNormal()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    x = Conv1D(80, kernel_size = 5, strides=1, padding=\"same\",dilation_ratew=1,kernel_initiaizer=initializer)(x)\n",
    "    x = tf.transpose(x,(0,2,1))\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    return Model(inputs= inp, output= x, name= \"posnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87871f8a",
   "metadata": {},
   "source": [
    "# 把上面三個組合起來, 這邊對應的是 model_vc 裡的 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e393ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVC(tf.keras.Model):\n",
    "    def __init__(self,dim_neck=32, dim_emb=256,len_crop=176,freq =22):\n",
    "        super(AutoVC,self).__init__()\n",
    "        self.encoder = Encoder((dim_emb+80,len_crop),dim_neck,dim_emb,freq)\n",
    "        self.decoder = Decoder((len_crop,320))\n",
    "        self.postnet = Posnet((len_crop,80))\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x = inputs[0]\n",
    "        c_org = inputs[1]\n",
    "        c_trg = inputs[-1]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        x = tf.transpose(x,(0,2,1))\n",
    "        c_org = tf.expand_dims(c_org,axis=1)\n",
    "        c_org = tf.transpose(tf.broadcast_to(c_org(tf.shape(c_org)[0],LEN_CROP,tf.shape(c_org)[-1])),(0,2,1))\n",
    "        # concat 80 那維\n",
    "        x = tf.concat([x,c_org],axis=1)\n",
    "        \n",
    "        codes = self.encoder(x)\n",
    "        if c_trg is None:\n",
    "            return tf.concat(codes,axis=-1)\n",
    "        \n",
    "        tmp = []\n",
    "        for code in codes:\n",
    "            tc = tf.expand_dim(code,axis=1)\n",
    "            tmp.append(tf.broadcast_to(tc,(batch_size,int(LEN_CROP/len(codes)),64)))\n",
    "        code_exp = tf.concat(tmp,axis=1)\n",
    "        \n",
    "        c_trg = tf.expand_dims(c_trg,axis=1)\n",
    "        c_trg = tf.broadcast_to(c_trg,(batch_size,tf.shape(x)[-1],DIM_EMB))\n",
    "        \n",
    "        # concat 64 那維\n",
    "        encoder_outputs = tf.concat((code_exp,c_trg), axis=-1)\n",
    "        mel_outputs = self.decoder(encoder_ouputs)\n",
    "        \n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = tf.transpose(mel_outputs_postnet,(0,2,1))\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs, mel_outputs_postnet, tf.concat(codes,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5da993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eea5834",
   "metadata": {},
   "source": [
    "# 生成 D_VECTOR\n",
    "\n",
    "下載 [Pre_train Model](https://drive.google.com/file/d/1ORAeb4DlS_65WDkQN6LHx5dPyCM5PAVV/view) 後定義 D_VECTOR, 這裡用的是 LSTM版的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102e98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.nn as nn\n",
    "\n",
    "class D_VECTOR(tf.keras.layers.Layer):\n",
    "    \"\"\" D_VECTOR speaker embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers=3, dim_input=40, dim_cell=256, dim_emb=64):\n",
    "        super(D_VECTOR, self).__init__()\n",
    "        self.lstm = tf.keras.layers.LSTM(input_size=dim_input, hidden_size=dim_cell,\n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "        self.embedding = tf.keras.layers.Linear(dim_cell, dim_emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        embeds = self.embedding(lstm_out[:,-1,:])\n",
    "        norm = embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "        embeds_normalized = embeds.div(norm)\n",
    "        return embeds_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b3d1cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[43mD_VECTOR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m      9\u001b[0m c_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3000000-BL.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     10\u001b[0m new_state_dict \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mD_VECTOR.__init__\u001b[1;34m(self, num_layers, dim_input, dim_cell, dim_emb)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, dim_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, dim_cell\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, dim_emb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28msuper\u001b[39m(D_VECTOR, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLinear(dim_cell, dim_emb)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# from model_bl import D_VECTOR\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "C = D_VECTOR(dim_input=80, dim_cell=768, dim_emb=256).eval().cpu()\n",
    "c_checkpoint = torch.load('3000000-BL.ckpt',map_location=torch.device('cpu'))\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for key, val in c_checkpoint['model_b'].items():\n",
    "    new_key = key[7:]\n",
    "    new_state_dict[new_key] = val\n",
    "    \n",
    "C.load_state_dict(new_state_dict)\n",
    "\n",
    "# 指的是說一個語者說了幾種不同內容的話, 讓資料的數量盡量一樣, 內容可以不一樣\n",
    "num_uttrs = 68\n",
    "len_crop = 176\n",
    "\n",
    "# Directory containing mel-spectrograms\n",
    "rootDir = './spmel'\n",
    "dirName, subdirList, _ = next(os.walk(rootDir))\n",
    "print('Found directory: %s' % dirName)\n",
    "\n",
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int=0):\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    \n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "    npad = [(0,0)] * array.ndim\n",
    "    npad[axis] = (0,pad_size)\n",
    "    \n",
    "    return np.pad(array, pad_size)\n",
    "\n",
    "speakers = []\n",
    "for speaker in sorted(subdirList[1:]):\n",
    "    print('Processing speaker: %s' % speaker)\n",
    "    utterances = []\n",
    "    utterances.append(speaker)\n",
    "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
    "    fileList = fileList[:num_uttrs]\n",
    "    \n",
    "    # make speaker embedding\n",
    "    assert len(fileList) >= num_uttrs\n",
    "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
    "    embs = []\n",
    "\n",
    "    for i in range(num_uttrs):\n",
    "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]),allow_pickle=True)\n",
    "        \n",
    "        # pad if the current one is too short\n",
    "        if tmp.shape[0] <= len_crop:\n",
    "            pad =int(len_crop - tmp.shape[0])\n",
    "            tmp = pad_along_axis(tmp,pad)\n",
    "            melsp = tf.convert_to_tensor(tmp[np.newaxis,:, :])\n",
    "        else:\n",
    "            left = np.random.randint(0, tmp.shape[0]-len_crop)\n",
    "            melsp = tf.convert_to_tensor(tmp[np.newaxis, left:left+len_crop, :])\n",
    "        \n",
    "        emb = C(melsp)\n",
    "        embs.append(emb,detach().squeeze.cpu().numpy())\n",
    "        \n",
    "utterance.append(np.mean(embs,axis=0))\n",
    "for fileName in sorted(fileList):\n",
    "    utterances.append(os.path.join(speaker,fileName))\n",
    "speakers.append(utterance)\n",
    "\n",
    "with open(os.path.join(rootDir,'train.pkl'),'wb') as handle:\n",
    "    pickle.dump(speakers,handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f507a08",
   "metadata": {},
   "source": [
    "# 製作 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7b3e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from multiprocessing import Process, Manager\n",
    "\n",
    "class Utterances(data.Dataset):\n",
    "    \"\"\"Data class for the Utterances dataset\"\"\"\n",
    "    \n",
    "    def __init__(self,root_dir,len_crop):\n",
    "        \"\"\"Initialize and preprocess the Utterances dataset.\"\"\"\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.len_crop = len_crop\n",
    "        self.step = 10\n",
    "        \n",
    "        metaname = self.root_dir+\"/train.pkl\"\n",
    "        print(metaname)\n",
    "        meta = pickle.load(open(metaname,'rb'))\n",
    "        \n",
    "        \"\"\"Load data using multiprocessing\"\"\"\n",
    "        manager = Manager()\n",
    "        meta = manager.list(meta)\n",
    "        dataset = manager.list(len(meta)*[None])\n",
    "        processes = []\n",
    "        \n",
    "        for i in range(0,len(meta), self.step):\n",
    "            p = Process(target=self.load_data,\n",
    "                        args =(meta[i:i+self.step],dataset,i))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            \n",
    "        self.train_dataset = list(dataset)\n",
    "        self.num_tokens = len(self.train_dataset)\n",
    "        \n",
    "        print('Finished loading the dataset...')\n",
    "        \n",
    "    def load_data(self,submeta,dataset,idx_offset):\n",
    "        for k, sbmt in enumerate(submeta):\n",
    "            uttrs = len(sbmt)*[None]\n",
    "            for j, tmp in enumerate(sbmt):\n",
    "                if j<2: # fill in speaker id and embedding\n",
    "                    uttrs[j] = tmp\n",
    "                else: # load the mel-spectrograms\n",
    "                    uttrs[j] = np.load(os.path.join(self.root_dir,tmp))\n",
    "            dataset[idx_offset+k] = uttrs\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        #pick a random speaker\n",
    "        dataset = self.train_dataset\n",
    "        list_uttrs =dataset[index]\n",
    "        emb_org = list_uttrs[1]\n",
    "        \n",
    "        # pick random uttr with random crop\n",
    "        a = np.random.randint(2,len(list_uttrs))\n",
    "        tmp = list_uttrs[a]\n",
    "        \n",
    "        if tmp.shape[0]<self.len_crop:\n",
    "            len_pad = self.len_crop - tmp.shape[0]\n",
    "            uttr = np.pad(tmp, ((0,len_pad),(0,0)),'constant')\n",
    "        elif tmp.shape[0]> self.len_crop:\n",
    "            left = np.random.randint(tmp.shape[0]-self.len_crop)\n",
    "            uttr = tmp[left:left+self.len_crop, :]\n",
    "        else:\n",
    "            uttr = tmp\n",
    "            \n",
    "        return uttr, emb_org\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of speakers.\"\"\"\n",
    "        return self.num_tokens\n",
    "    \n",
    "def get_loader(root_dir,batch_size=2, len_crop=176,num_workers=0):\n",
    "    \"\"\"Build and return a data loader.\"\"\"\n",
    "    \n",
    "    dataset = Utterances(root_dir,len_crop)\n",
    "    \n",
    "    worker_init_fn = lambda x: np.random.seed((torch.initial_seed()) % (2**32))\n",
    "    data_loader = data.DataLoader(dataset=dataset,\n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers = num_workers,\n",
    "                                  drop_last=True,\n",
    "                                  worker_init_fn = worker_init_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d4e28",
   "metadata": {},
   "source": [
    "# Train-Loop\n",
    "\n",
    "* 驗證的話至少要訓練 30000 次才聽得出來, 在 2080Ti 要跑約 6 小時, 要聽到不錯的結果要訓練 1000000次 , loop 會收斂到 0.0001\n",
    "\n",
    "\n",
    "Tensorflow 版的要 DIM_NECK 設 44 才會 work , 目前原因不明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f66ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./spmel/train.pkl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './spmel/train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m LEN_CROP \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 3\u001b[0m vcc_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./spmel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLEN_CROP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m....Strat....\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mget_loader\u001b[1;34m(root_dir, batch_size, len_crop, num_workers)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loader\u001b[39m(root_dir,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, len_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m176\u001b[39m,num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124;03m\"\"\"Build and return a data loader.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mUtterances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlen_crop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     worker_init_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed((torch\u001b[38;5;241m.\u001b[39minitial_seed()) \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m     83\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m     84\u001b[0m                                   batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[0;32m     85\u001b[0m                                   shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     86\u001b[0m                                   num_workers \u001b[38;5;241m=\u001b[39m num_workers,\n\u001b[0;32m     87\u001b[0m                                   drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m                                   worker_init_fn \u001b[38;5;241m=\u001b[39m worker_init_fn)\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mUtterances.__init__\u001b[1;34m(self, root_dir, len_crop)\u001b[0m\n\u001b[0;32m     18\u001b[0m metaname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(metaname)\n\u001b[1;32m---> 20\u001b[0m meta \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetaname\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"Load data using multiprocessing\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m manager \u001b[38;5;241m=\u001b[39m Manager()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './spmel/train.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 100\n",
    "LEN_CROP = 50\n",
    "vcc_loader = get_loader('./spmel',BATCH_SIZE,LEN_CROP)\n",
    "\n",
    "\n",
    "def train(step=30000, batch_size =2):\n",
    "    print('....Strat....')\n",
    "    \n",
    "    for j in range(step):\n",
    "        \n",
    "        # 這裡的跟之前 pytorch 那邊一樣, 所以要確保輸入跟 pytorch 一樣\n",
    "        try:\n",
    "            x_real, emb_org = next(data_iter)\n",
    "        except:\n",
    "            data_iter = iter(vcc_loader)\n",
    "            x_real, emb_org = next(data_iter)\n",
    "            \n",
    "        # 因為輸入資料是 torch tensor 記得要轉回 np\n",
    "        x_real = x_real.detach().cpu().numpy().astype(np.float32)\n",
    "        emb_org = emb_org.detach().cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        # train_step 見下方\n",
    "        g_loss_id, g_loss_id_psnt, g_loss_cd = train_step(x_real,emb_org,emb_org)\n",
    "        \n",
    "        if (j+1)%10 ==0:\n",
    "            print(f\"Step:{j}\")\n",
    "            print(f\"G_loss_id:{g_loss_id}\")\n",
    "            print(f\"G_loss_id_psnet:{g_loss_id_psnt}\")\n",
    "            print(f\"G_loss_cd:{g_loss_cd}\")\n",
    "        if (j+2)%10 ==0:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        # 什麼時候存都可以, 使用的時候就 autovc.encoder.load_weight('encoder_weights') 載入就好\n",
    "        \n",
    "        if (j+1)%10000==0:\n",
    "            autovc.encoder.save_weights(f\"model/encoder_weights_step_{j+1}.h5\")\n",
    "            autovc.decoder.save_weights(f\"model/decoder_weights_step_{j+1}.h5\")\n",
    "            autovc.postnet.save_weights(f\"model/postnet_weights_step_{j+1}.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d486fad",
   "metadata": {},
   "source": [
    "# Train-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97407819",
   "metadata": {},
   "outputs": [],
   "source": [
    "autovc_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_real,emb_org,emb_trg):\n",
    "    # tf.GradientTape() 等價於 loss.backward()\n",
    "    \n",
    "    with tf.GradientTape() as autovc_tape:\n",
    "        x_identic, x_identic_psnt, code_real = autovc([x_real,emb_org,emb_trg])\n",
    "        \n",
    "        # loss 請參考昨天那篇\n",
    "        g_loss_id = mse_loss(x_real, x_identic)\n",
    "        g_loss_id_psnt = mse_loss(x_real, x_identic_psnt)\n",
    "        \n",
    "        code_reconst = autovc([x_identic_psnt, emb_org, None])\n",
    "        \n",
    "        g_loss_cd = l1_loss(code_real, code_reconst)\n",
    "        g_loss = g_loss_id + g_loss_id_psnt + g_loss_cd\n",
    "        \n",
    "    gradients_of_autovc = autovc_tape.gradient(g_loss,autovc.trainable_variables)\n",
    "    autovc_optimizer.apply_gradients(zip(gradients_of_autovc.trainable_variable))\n",
    "    \n",
    "    return g_loss_id, g_loss_id_psnt, g_loss_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e40822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Strat....\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vcc_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(step, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     x_real, emb_org \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mdata_iter\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'data_iter' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 接下來即可開始訓練\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 到這邊我們已經把 AutoVC 做過兩遍了，TF 做出來的效果跟 Pytorch 的是一樣的，只是 dim_neck 這個參數比較令人疑惑，為什麼 pytorch 的可以在 32 上成功但 TF 的不行，但兩邊在 freq = 22, dim_neck = 44 的情況下轉出來的聲音效果我聽起來是差不多的。\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m900000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(step, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m     x_real, emb_org \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mvcc_loader\u001b[49m)\n\u001b[0;32m     11\u001b[0m     x_real, emb_org \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iter)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 因為輸入資料是 torch tensor 記得要轉回 np\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vcc_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# 接下來即可開始訓練\n",
    "# 到這邊我們已經把 AutoVC 做過兩遍了，TF 做出來的效果跟 Pytorch 的是一樣的，只是 dim_neck 這個參數比較令人疑惑，為什麼 pytorch 的可以在 32 上成功但 TF 的不行，但兩邊在 freq = 22, dim_neck = 44 的情況下轉出來的聲音效果我聽起來是差不多的。\n",
    "\n",
    "train(900000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482f515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
